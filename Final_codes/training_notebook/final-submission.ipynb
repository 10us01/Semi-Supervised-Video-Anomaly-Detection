{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. INSTALL\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# NOTE: Internet access is required here for:\n# 1. Installing external libraries (CLIP, ultralytics, thop).\n# 2. Downloading the YOLOv5s model weights (via torch.hub in Cell 3).\n# 3. Downloading the CLIP ResNet101 model weights (via clip.load in Cell 3).\nprint(\"âš™ï¸ Installing Dependencies...\")\nos.system(\"pip install git+https://github.com/openai/CLIP.git -q\")\nos.system(\"pip install -U seaborn thop ultralytics scikit-learn -q\")\n\n# 2. IMPORTS\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nfrom sklearn.metrics import pairwise_distances\nimport clip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Dataset Paths\n    DATASET_ROOT = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset'\n    TRAIN_FOLDER = 'training_videos'\n    TEST_FOLDER = 'testing_videos'\n    \n    # Hyperparameters\n    CLIP_ARCH = \"RN101\"        \n    YOLO_MODEL = 'yolov5s'     # Using Small model (Stable & Fast)\n    \n    IMG_SIZE = (224, 224)\n    CORESET_RATIO = 0.01       # 1% Memory Retention\n    \n    # Ensemble & Fusion\n    SEEDS = [42, 123, 2024]    \n    W_LOCAL = 0.7              \n    W_GLOBAL = 0.3             \n    SMOOTHING_SIGMA = 17       \n    CONF_THRES = 0.5           \n    \n    WORK_DIR = './working_dir'\n\nos.makedirs(CFG.WORK_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Object Detector (YOLOv5) ---\nclass PersonDetector:\n    def __init__(self, device):\n        print(f\"   > Loading {CFG.YOLO_MODEL}...\")\n        self.model = torch.hub.load('ultralytics/yolov5', CFG.YOLO_MODEL, pretrained=True, verbose=False)\n        self.model.to(device)\n        self.model.classes = [0] # Class 0 = Person\n        self.model.conf = CFG.CONF_THRES\n\n    def detect_and_crop(self, img_path):\n        results = self.model(img_path)\n        crops = []\n        if len(results.xyxy[0]) > 0:\n            img = Image.open(img_path).convert('RGB')\n            for *box, conf, cls in results.xyxy[0]:\n                x1, y1, x2, y2 = map(int, box)\n                c = img.crop((x1, y1, x2, y2)).resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n                crops.append(c)\n        return crops\n\n# --- Feature Extractor (CLIP) ---\nclass VideoPatchCore(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        print(f\"   > Loading CLIP {CFG.CLIP_ARCH}...\")\n        self.model, self.preprocess = clip.load(CFG.CLIP_ARCH, device=device)\n        self.model.eval()\n        self.features = {}\n        # Hook into intermediate layers\n        self.model.visual.layer2.register_forward_hook(self.hook('layer2'))\n        self.model.visual.layer3.register_forward_hook(self.hook('layer3'))\n\n    def hook(self, name):\n        def fn(model, input, output): self.features[name] = output\n        return fn\n\n    def embed(self, pil_images):\n        if not pil_images: return None\n        batch = torch.stack([self.preprocess(img) for img in pil_images]).to(CFG.DEVICE)\n        \n        with torch.no_grad():\n            self.model.visual(batch)\n        \n        l2, l3 = self.features['layer2'], self.features['layer3']\n        if l2.shape[-2:] != l3.shape[-2:]:\n            l2 = F.interpolate(l2, size=l3.shape[-2:], mode='bilinear', align_corners=False)\n        \n        # Concatenate and Average Pool to 1x1 vector (No Patching)\n        embedding = torch.cat([l2, l3], dim=1)\n        vector = F.adaptive_avg_pool2d(embedding, 1).flatten(1)\n        return vector.cpu()\n\n# --- Memory Optimization (Coreset) ---\ndef subsample(features, ratio=0.01, seed=42):\n    np.random.seed(seed)\n    \n    if len(features) > 20000:\n        indices = np.random.choice(len(features), 20000, replace=False)\n        features = features[indices]\n\n    num_samples = max(1, int(len(features) * ratio))\n    features_np = features.float().numpy() \n    \n    # Greedy K-Center Selection\n    current_idx = np.random.choice(len(features_np))\n    selected = [current_idx]\n    min_dists = pairwise_distances(features_np[current_idx:current_idx+1], features_np).flatten()\n    \n    for _ in range(1, num_samples):\n        new_idx = np.argmax(min_dists)\n        selected.append(new_idx)\n        new_dists = pairwise_distances(features_np[new_idx:new_idx+1], features_np).flatten()\n        min_dists = np.minimum(min_dists, new_dists)\n        \n    return features[selected].to(CFG.DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_pipeline():\n    detector = PersonDetector(CFG.DEVICE)\n    vpc = VideoPatchCore(CFG.DEVICE)\n    \n    # ---------------------------------------------------------\n    # STEP 1: EXTRACT FEATURES (Memorization)\n    # ---------------------------------------------------------\n    print(\"\\n[PHASE 1] Extracting Features from Training Data...\")\n    train_path = os.path.join(CFG.DATASET_ROOT, CFG.TRAIN_FOLDER)\n    video_folders = sorted(glob.glob(os.path.join(train_path, '*')))\n    \n    local_cache = []\n    global_cache = []\n    \n    print(f\"   > Scanning {len(video_folders)} videos...\")\n    for video_dir in tqdm(video_folders):\n        frames = sorted(glob.glob(os.path.join(video_dir, '*.jpg')))\n        \n        # Subsample every 5th frame for speed\n        for frame_path in frames[::5]: \n            # Global Feature (Whole Frame)\n            img = Image.open(frame_path).convert('RGB')\n            global_img = img.resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n            global_cache.append(vpc.embed([global_img]))\n            \n            # Local Features (People Crops)\n            crops = detector.detect_and_crop(frame_path)\n            if crops:\n                local_cache.append(vpc.embed(crops))\n\n    if not local_cache:\n        raise ValueError(\"CRITICAL: No people detected in training data.\")\n\n    full_local = torch.cat(local_cache, dim=0)\n    full_global = torch.cat(global_cache, dim=0)\n    print(f\"   > Extracted: {len(full_local)} Local vectors, {len(full_global)} Global vectors.\")\n\n    # ---------------------------------------------------------\n    # STEP 2: ENSEMBLE INFERENCE (3 Seeds)\n    # ---------------------------------------------------------\n    final_preds = {} \n    \n    print(\"\\n[PHASE 2] Starting Ensemble Inference...\")\n    test_path = os.path.join(CFG.DATASET_ROOT, CFG.TEST_FOLDER)\n    test_videos = sorted(glob.glob(os.path.join(test_path, '*')))\n    \n    # Pre-load all test frame paths\n    all_test_frames = []\n    for video_dir in test_videos:\n        vid_id = os.path.basename(video_dir)\n        frames = sorted(glob.glob(os.path.join(video_dir, '*.jpg')))\n        for f in frames:\n            all_test_frames.append((vid_id, f))\n            # Init prediction storage\n            fname = os.path.basename(f)\n            try: fid = f\"{int(vid_id)}_{int(fname.split('_')[1].split('.')[0])}\"\n            except: fid = f\"{vid_id}_{fname}\"\n            final_preds[fid] = 0.0\n\n    # Ensemble Loop\n    for i, seed in enumerate(CFG.SEEDS):\n        print(f\"\\n--- RUN {i+1}/{len(CFG.SEEDS)} (Seed {seed}) ---\")\n        \n        # Build specific memory bank for this seed\n        local_bank = subsample(full_local, CFG.CORESET_RATIO, seed=seed)\n        global_bank = subsample(full_global, CFG.CORESET_RATIO, seed=seed)\n        \n        # Score Test Data\n        print(f\"   > Scoring...\")\n        for vid_id, frame_path in tqdm(all_test_frames):\n            fname = os.path.basename(frame_path)\n            try: fid = f\"{int(vid_id)}_{int(fname.split('_')[1].split('.')[0])}\"\n            except: fid = f\"{vid_id}_{fname}\"\n                \n            # Global Score (Nearest Neighbor)\n            img = Image.open(frame_path).convert('RGB')\n            global_img = img.resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n            g_vec = vpc.embed([global_img]).to(CFG.DEVICE)\n            d_global = torch.cdist(g_vec.float(), global_bank.float()).min(dim=1)[0].item()\n            \n            # Local Score (Max anomaly of any person in frame)\n            crops = detector.detect_and_crop(frame_path)\n            if len(crops) == 0: \n                d_local = 0.0\n            else:\n                l_vecs = vpc.embed(crops).to(CFG.DEVICE)\n                d_local = torch.cdist(l_vecs.float(), local_bank.float()).min(dim=1)[0].max().item()\n            \n            # Weighted Fusion\n            score = (CFG.W_LOCAL * d_local) + (CFG.W_GLOBAL * d_global)\n            final_preds[fid] += score\n\n    # Average scores across seeds\n    for k in final_preds: \n        final_preds[k] /= len(CFG.SEEDS)\n        \n    return final_preds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    results = run_pipeline()\n    \n    print(\"\\n[PHASE 3] ðŸ“ Generating Submission...\")\n    df = pd.DataFrame(list(results.items()), columns=['Id', 'Raw_Score'])\n    \n    # 1. Robust Sorting (Required for smoothing)\n    try:\n        df['VidID'] = df['Id'].apply(lambda x: int(x.split('_')[0]))\n        df['FrameID'] = df['Id'].apply(lambda x: int(x.split('_')[1]))\n        df = df.sort_values(['VidID', 'FrameID'])\n    except:\n        print(\"âš ï¸ Warning: Non-standard ID format. Sorting alphanumerically.\")\n        df = df.sort_values('Id')\n    \n    # 2. Gaussian Smoothing\n    df['Predicted'] = gaussian_filter1d(df['Raw_Score'], sigma=CFG.SMOOTHING_SIGMA)\n    \n    # 3. Min-Max Normalization\n    _min, _max = df['Predicted'].min(), df['Predicted'].max()\n    if _max > _min:\n        df['Predicted'] = (df['Predicted'] - _min) / (_max - _min)\n    else:\n        df['Predicted'] = 0.0\n        \n    # 4. Save Final CSV\n    submission = df[['Id', 'Predicted']]\n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"âœ… DONE. Submission 'submission.csv' ready.\")\n    print(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}