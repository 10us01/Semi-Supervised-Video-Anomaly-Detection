{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. INSTALL\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# NOTE: Internet access is required here for:\n# 1. Installing external libraries (CLIP, ultralytics, thop).\n# 2. Downloading the YOLOv5s model weights (via torch.hub in Cell 3).\n# 3. Downloading the CLIP ResNet101 model weights (via clip.load in Cell 3).\nprint(\"âš™ï¸ Installing Dependencies...\")\nos.system(\"pip install git+https://github.com/openai/CLIP.git -q\")\nos.system(\"pip install -U seaborn thop ultralytics scikit-learn -q\")\n\n# 2. IMPORTS\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom scipy.ndimage import gaussian_filter1d\nfrom sklearn.metrics import pairwise_distances\nimport clip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T07:01:20.166093Z","iopub.execute_input":"2026-01-08T07:01:20.166270Z","iopub.status.idle":"2026-01-08T07:01:44.728196Z","shell.execute_reply.started":"2026-01-08T07:01:20.166251Z","shell.execute_reply":"2026-01-08T07:01:44.727559Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸ Installing Dependencies...\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 44.8/44.8 kB 1.9 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 32.3 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8.9/8.9 MB 134.2 MB/s eta 0:00:00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class CFG:\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Dataset Paths\n    DATASET_ROOT = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset'\n    TRAIN_FOLDER = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\n    TEST_FOLDER = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n    \n    # Hyperparameters\n    CLIP_ARCH = \"RN101\"        \n    YOLO_MODEL = 'yolov5s'     # Using Small model (Stable & Fast)\n    \n    IMG_SIZE = (224, 224)\n    CORESET_RATIO = 0.01       # 1% Memory Retention\n    \n    # Ensemble & Fusion\n    SEEDS = [42, 123, 2024]    \n    W_LOCAL = 0.7              \n    W_GLOBAL = 0.3             \n    SMOOTHING_SIGMA = 17       \n    CONF_THRES = 0.5           \n    \n    WORK_DIR = './working_dir'\n\nos.makedirs(CFG.WORK_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T07:01:50.296334Z","iopub.execute_input":"2026-01-08T07:01:50.296626Z","iopub.status.idle":"2026-01-08T07:01:50.301940Z","shell.execute_reply.started":"2026-01-08T07:01:50.296599Z","shell.execute_reply":"2026-01-08T07:01:50.301207Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- Object Detector (YOLOv5) ---\nclass PersonDetector:\n    def __init__(self, device):\n        print(f\"   > Loading {CFG.YOLO_MODEL}...\")\n        self.model = torch.hub.load('ultralytics/yolov5', CFG.YOLO_MODEL, pretrained=True, verbose=False)\n        self.model.to(device)\n        self.model.classes = [0] # Class 0 = Person\n        self.model.conf = CFG.CONF_THRES\n\n    def detect_and_crop(self, img_path):\n        results = self.model(img_path)\n        crops = []\n        if len(results.xyxy[0]) > 0:\n            img = Image.open(img_path).convert('RGB')\n            for *box, conf, cls in results.xyxy[0]:\n                x1, y1, x2, y2 = map(int, box)\n                c = img.crop((x1, y1, x2, y2)).resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n                crops.append(c)\n        return crops\n\n# --- Feature Extractor (CLIP) ---\nclass VideoPatchCore(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        print(f\"   > Loading CLIP {CFG.CLIP_ARCH}...\")\n        self.model, self.preprocess = clip.load(CFG.CLIP_ARCH, device=device)\n        self.model.eval()\n        self.features = {}\n        # Hook into intermediate layers\n        self.model.visual.layer2.register_forward_hook(self.hook('layer2'))\n        self.model.visual.layer3.register_forward_hook(self.hook('layer3'))\n\n    def hook(self, name):\n        def fn(model, input, output): self.features[name] = output\n        return fn\n\n    def embed(self, pil_images):\n        if not pil_images: return None\n        batch = torch.stack([self.preprocess(img) for img in pil_images]).to(CFG.DEVICE)\n        \n        with torch.no_grad():\n            self.model.visual(batch)\n        \n        l2, l3 = self.features['layer2'], self.features['layer3']\n        if l2.shape[-2:] != l3.shape[-2:]:\n            l2 = F.interpolate(l2, size=l3.shape[-2:], mode='bilinear', align_corners=False)\n        \n        # Concatenate and Average Pool to 1x1 vector (No Patching)\n        embedding = torch.cat([l2, l3], dim=1)\n        vector = F.adaptive_avg_pool2d(embedding, 1).flatten(1)\n        return vector.cpu()\n\n# --- Memory Optimization (Coreset) ---\ndef subsample(features, ratio=0.01, seed=42):\n    np.random.seed(seed)\n    \n    if len(features) > 20000:\n        indices = np.random.choice(len(features), 20000, replace=False)\n        features = features[indices]\n\n    num_samples = max(1, int(len(features) * ratio))\n    features_np = features.float().numpy() \n    \n    # Greedy K-Center Selection\n    current_idx = np.random.choice(len(features_np))\n    selected = [current_idx]\n    min_dists = pairwise_distances(features_np[current_idx:current_idx+1], features_np).flatten()\n    \n    for _ in range(1, num_samples):\n        new_idx = np.argmax(min_dists)\n        selected.append(new_idx)\n        new_dists = pairwise_distances(features_np[new_idx:new_idx+1], features_np).flatten()\n        min_dists = np.minimum(min_dists, new_dists)\n        \n    return features[selected].to(CFG.DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T07:01:55.338137Z","iopub.execute_input":"2026-01-08T07:01:55.338912Z","iopub.status.idle":"2026-01-08T07:01:55.354613Z","shell.execute_reply.started":"2026-01-08T07:01:55.338882Z","shell.execute_reply":"2026-01-08T07:01:55.353990Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def run_pipeline():\n    detector = PersonDetector(CFG.DEVICE)\n    vpc = VideoPatchCore(CFG.DEVICE)\n    \n    # ---------------------------------------------------------\n    # STEP 1: EXTRACT FEATURES (Memorization)\n    # ---------------------------------------------------------\n    print(\"\\n[PHASE 1] Extracting Features from Training Data...\")\n    train_path = os.path.join(CFG.DATASET_ROOT, CFG.TRAIN_FOLDER)\n    video_folders = sorted(glob.glob(os.path.join(train_path, '*')))\n    \n    local_cache = []\n    global_cache = []\n    \n    print(f\"   > Scanning {len(video_folders)} videos...\")\n    for video_dir in tqdm(video_folders):\n        frames = sorted(glob.glob(os.path.join(video_dir, '*.jpg')))\n        \n        # Subsample every 5th frame for speed\n        for frame_path in frames[::5]: \n            # Global Feature (Whole Frame)\n            img = Image.open(frame_path).convert('RGB')\n            global_img = img.resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n            global_cache.append(vpc.embed([global_img]))\n            \n            # Local Features (People Crops)\n            crops = detector.detect_and_crop(frame_path)\n            if crops:\n                local_cache.append(vpc.embed(crops))\n\n    if not local_cache:\n        raise ValueError(\"CRITICAL: No people detected in training data.\")\n\n    full_local = torch.cat(local_cache, dim=0)\n    full_global = torch.cat(global_cache, dim=0)\n    print(f\"   > Extracted: {len(full_local)} Local vectors, {len(full_global)} Global vectors.\")\n\n    # ---------------------------------------------------------\n    # STEP 2: ENSEMBLE INFERENCE (3 Seeds)\n    # ---------------------------------------------------------\n    final_preds = {} \n    \n    print(\"\\n[PHASE 2] Starting Ensemble Inference...\")\n    test_path = os.path.join(CFG.DATASET_ROOT, CFG.TEST_FOLDER)\n    test_videos = sorted(glob.glob(os.path.join(test_path, '*')))\n    \n    # Pre-load all test frame paths\n    all_test_frames = []\n    for video_dir in test_videos:\n        vid_id = os.path.basename(video_dir)\n        frames = sorted(glob.glob(os.path.join(video_dir, '*.jpg')))\n        for f in frames:\n            all_test_frames.append((vid_id, f))\n            # Init prediction storage\n            fname = os.path.basename(f)\n            try: fid = f\"{int(vid_id)}_{int(fname.split('_')[1].split('.')[0])}\"\n            except: fid = f\"{vid_id}_{fname}\"\n            final_preds[fid] = 0.0\n\n    # Ensemble Loop\n    for i, seed in enumerate(CFG.SEEDS):\n        print(f\"\\n--- RUN {i+1}/{len(CFG.SEEDS)} (Seed {seed}) ---\")\n        \n        # Build specific memory bank for this seed\n        local_bank = subsample(full_local, CFG.CORESET_RATIO, seed=seed)\n        global_bank = subsample(full_global, CFG.CORESET_RATIO, seed=seed)\n        \n        # Score Test Data\n        print(f\"   > Scoring...\")\n        for vid_id, frame_path in tqdm(all_test_frames):\n            fname = os.path.basename(frame_path)\n            try: fid = f\"{int(vid_id)}_{int(fname.split('_')[1].split('.')[0])}\"\n            except: fid = f\"{vid_id}_{fname}\"\n                \n            # Global Score (Nearest Neighbor)\n            img = Image.open(frame_path).convert('RGB')\n            global_img = img.resize(CFG.IMG_SIZE, Image.Resampling.BILINEAR)\n            g_vec = vpc.embed([global_img]).to(CFG.DEVICE)\n            d_global = torch.cdist(g_vec.float(), global_bank.float()).min(dim=1)[0].item()\n            \n            # Local Score (Max anomaly of any person in frame)\n            crops = detector.detect_and_crop(frame_path)\n            if len(crops) == 0: \n                d_local = 0.0\n            else:\n                l_vecs = vpc.embed(crops).to(CFG.DEVICE)\n                d_local = torch.cdist(l_vecs.float(), local_bank.float()).min(dim=1)[0].max().item()\n            \n            # Weighted Fusion\n            score = (CFG.W_LOCAL * d_local) + (CFG.W_GLOBAL * d_global)\n            final_preds[fid] += score\n\n    # Average scores across seeds\n    for k in final_preds: \n        final_preds[k] /= len(CFG.SEEDS)\n        \n    return final_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T07:01:59.760225Z","iopub.execute_input":"2026-01-08T07:01:59.760861Z","iopub.status.idle":"2026-01-08T07:01:59.773886Z","shell.execute_reply.started":"2026-01-08T07:01:59.760832Z","shell.execute_reply":"2026-01-08T07:01:59.773177Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    results = run_pipeline()\n    \n    print(\"\\n[PHASE 3] ðŸ“ Generating Submission...\")\n    df = pd.DataFrame(list(results.items()), columns=['Id', 'Raw_Score'])\n    \n    # 1. Robust Sorting (Required for smoothing)\n    try:\n        df['VidID'] = df['Id'].apply(lambda x: int(x.split('_')[0]))\n        df['FrameID'] = df['Id'].apply(lambda x: int(x.split('_')[1]))\n        df = df.sort_values(['VidID', 'FrameID'])\n    except:\n        print(\"âš ï¸ Warning: Non-standard ID format. Sorting alphanumerically.\")\n        df = df.sort_values('Id')\n    \n    # 2. Gaussian Smoothing\n    df['Predicted'] = gaussian_filter1d(df['Raw_Score'], sigma=CFG.SMOOTHING_SIGMA)\n    \n    # 3. Min-Max Normalization\n    _min, _max = df['Predicted'].min(), df['Predicted'].max()\n    if _max > _min:\n        df['Predicted'] = (df['Predicted'] - _min) / (_max - _min)\n    else:\n        df['Predicted'] = 0.0\n        \n    # 4. Save Final CSV\n    submission = df[['Id', 'Predicted']]\n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"âœ… DONE. Submission 'submission.csv' ready.\")\n    print(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T07:02:05.404658Z","iopub.execute_input":"2026-01-08T07:02:05.405416Z","iopub.status.idle":"2026-01-08T07:41:02.641190Z","shell.execute_reply.started":"2026-01-08T07:02:05.405387Z","shell.execute_reply":"2026-01-08T07:41:02.640521Z"}},"outputs":[{"name":"stdout","text":"   > Loading yolov5s...\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\nCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"YOLOv5 ðŸš€ 2026-1-8 Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 118MB/s] \n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\nAdding AutoShape... \n","output_type":"stream"},{"name":"stdout","text":"   > Loading CLIP RN101...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278M/278M [00:04<00:00, 69.1MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[PHASE 1] Extracting Features from Training Data...\n   > Scanning 16 videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [02:18<00:00,  8.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"   > Extracted: 11989 Local vectors, 1847 Global vectors.\n\n[PHASE 2] Starting Ensemble Inference...\n\n--- RUN 1/3 (Seed 42) ---\n   > Scoring...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11706/11706 [13:16<00:00, 14.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- RUN 2/3 (Seed 123) ---\n   > Scoring...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11706/11706 [11:42<00:00, 16.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- RUN 3/3 (Seed 2024) ---\n   > Scoring...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11706/11706 [11:09<00:00, 17.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n[PHASE 3] ðŸ“ Generating Submission...\nâœ… DONE. Submission 'submission.csv' ready.\n      Id  Predicted\n0  1_939   1.000000\n1  1_940   0.999914\n2  1_941   0.999740\n3  1_942   0.999479\n4  1_943   0.999133\n","output_type":"stream"}],"execution_count":5}]}