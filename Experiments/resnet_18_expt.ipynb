{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport re\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom scipy.stats import rankdata\n\n# Use GPU for PyTorch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# --- AUTO-FIND PATHS ---\nTRAIN_DIR = \"\"\nTEST_DIR = \"\"\nfor root, dirs, files in os.walk('/kaggle/input'):\n    if \"training_videos\" in dirs:\n        TRAIN_DIR = os.path.join(root, 'training_videos')\n    if \"testing_videos\" in dirs:\n        TEST_DIR = os.path.join(root, 'testing_videos')\n\nif TRAIN_DIR and TEST_DIR:\n    print(f\"âœ… Found Dataset:\\nTrain: {TRAIN_DIR}\\nTest:  {TEST_DIR}\")\nelse:\n    print(\"âŒ ERROR: Dataset paths not found!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T20:55:13.054170Z","iopub.execute_input":"2026-01-05T20:55:13.054436Z","iopub.status.idle":"2026-01-05T20:56:09.875937Z","shell.execute_reply.started":"2026-01-05T20:55:13.054404Z","shell.execute_reply":"2026-01-05T20:56:09.875242Z"}},"outputs":[{"name":"stderr","text":"2026-01-05 20:55:22.930934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767646523.124280      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767646523.177163      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767646523.641050      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767646523.641089      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767646523.641091      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767646523.641094      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nâœ… Found Dataset:\nTrain: /kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos\nTest:  /kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- BUILD 2D EXTRACTOR ---\nprint(\"Loading ResNet18 (ImageNet)...\")\nweights = ResNet18_Weights.IMAGENET1K_V1\nmodel_2d = resnet18(weights=weights).to(device)\n# Remove classification layer to get 512-dim features\nmodel_2d.fc = nn.Identity()\nmodel_2d.eval()\n\n# Standard ImageNet Transform\ntransform_2d = weights.transforms()\n\ndef extract_features_from_folder(folder_path):\n    \"\"\"\n    Reads all images in a folder and returns their 512-dim features.\n    Returns: (N, 512) array, List of paths\n    \"\"\"\n    image_paths = sorted(glob.glob(os.path.join(folder_path, \"*\")))\n    if len(image_paths) == 0: return None, None\n    \n    frames = []\n    for p in image_paths:\n        img = cv2.imread(p)\n        if img is not None:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            frames.append(img)\n            \n    if len(frames) == 0: return None, None\n    \n    # Process in batches to avoid OOM\n    batch_size = 128\n    feats = []\n    \n    for i in range(0, len(frames), batch_size):\n        batch_imgs = frames[i:i+batch_size]\n        \n        # Preprocess: List of Arrays -> Stack -> To Tensor -> Transform\n        # Note: transform_2d expects (C,H,W), but cv2 gives (H,W,C). \n        # We must permute BEFORE transform.\n        batch_tensor = torch.stack([\n            transform_2d(torch.from_numpy(img).permute(2,0,1)) \n            for img in batch_imgs\n        ]).to(device)\n        \n        with torch.no_grad():\n            f = model_2d(batch_tensor)\n            feats.append(f.cpu().numpy())\n            \n    return np.vstack(feats), image_paths\n\nprint(\"âœ… 2D Extractor Ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T20:56:13.696885Z","iopub.execute_input":"2026-01-05T20:56:13.697456Z","iopub.status.idle":"2026-01-05T20:56:14.336135Z","shell.execute_reply.started":"2026-01-05T20:56:13.697426Z","shell.execute_reply":"2026-01-05T20:56:14.335476Z"}},"outputs":[{"name":"stdout","text":"Loading ResNet18 (ImageNet)...\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 238MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… 2D Extractor Ready!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- 1. EXTRACT TRAIN FEATURES ---\nprint(\"ğŸš€ Phase 1: Extracting 2D Features from Training Videos...\")\ntrain_folders = sorted(glob.glob(os.path.join(TRAIN_DIR, \"*\")))\n\nall_train_feats = []\n\nfor folder in tqdm(train_folders):\n    f, _ = extract_features_from_folder(folder)\n    if f is not None:\n        all_train_feats.append(f)\n\n# Stack all features: (Total_Train_Frames, 512)\nX_train = np.vstack(all_train_feats)\nprint(f\"Training Data Shape: {X_train.shape}\")\n\n# --- 2. BUILD DAE MODEL ---\n# Input dim is 512 (ResNet18 output)\nINPUT_DIM = 512 \n\ndef build_dae(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    \n    # Noise\n    x = layers.GaussianNoise(0.1)(inputs)\n    \n    # Encoder\n    x = layers.Dense(256)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    \n    x = layers.Dense(64)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    \n    # Bottleneck\n    code = layers.Dense(32)(x)\n    x = layers.BatchNormalization()(code)\n    code = layers.Activation('relu')(x)\n    \n    # Decoder\n    x = layers.Dense(64)(code)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    \n    x = layers.Dense(256)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    \n    # Output\n    outputs = layers.Dense(input_dim, activation='linear')(x)\n    \n    model = models.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\ndae = build_dae(INPUT_DIM)\n\n# --- 3. TRAIN ---\nprint(\"ğŸš€ Phase 2: Training DAE...\")\ndae.fit(\n    X_train, X_train,\n    epochs=50,\n    batch_size=128,\n    shuffle=True,\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T20:56:21.233786Z","iopub.execute_input":"2026-01-05T20:56:21.234410Z","iopub.status.idle":"2026-01-05T20:58:26.692848Z","shell.execute_reply.started":"2026-01-05T20:56:21.234382Z","shell.execute_reply":"2026-01-05T20:58:26.692170Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Phase 1: Extracting 2D Features from Training Videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:45<00:00,  6.58s/it]\nI0000 00:00:1767646686.560303      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14161 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Training Data Shape: (9204, 512)\nğŸš€ Phase 2: Training DAE...\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1767646691.037352     128 service.cc:152] XLA service 0x7e425c008390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1767646691.037385     128 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1767646691.635006     128 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m61/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4855","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767646693.640013     128 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 0.4420\nEpoch 2/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0545\nEpoch 3/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0487\nEpoch 4/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0448\nEpoch 5/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0421\nEpoch 6/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0404\nEpoch 7/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0388\nEpoch 8/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0368\nEpoch 9/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0362\nEpoch 10/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0345\nEpoch 11/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0341\nEpoch 12/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0332\nEpoch 13/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0322\nEpoch 14/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0305\nEpoch 15/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0305\nEpoch 16/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0298\nEpoch 17/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0292\nEpoch 18/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0290\nEpoch 19/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0282\nEpoch 20/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0276\nEpoch 21/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0277\nEpoch 22/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0270\nEpoch 23/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0267\nEpoch 24/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0265\nEpoch 25/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0265\nEpoch 26/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0262\nEpoch 27/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0256\nEpoch 28/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0251\nEpoch 29/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0249\nEpoch 30/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0249\nEpoch 31/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0250\nEpoch 32/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0245\nEpoch 33/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0241\nEpoch 34/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0240\nEpoch 35/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0239\nEpoch 36/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0235\nEpoch 37/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0237\nEpoch 38/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0232\nEpoch 39/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0235\nEpoch 40/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0233\nEpoch 41/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0232\nEpoch 42/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0231\nEpoch 43/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0228\nEpoch 44/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0227\nEpoch 45/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0226\nEpoch 46/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0228\nEpoch 47/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0226\nEpoch 48/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0220\nEpoch 49/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0225\nEpoch 50/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0225\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7e436abc31a0>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# --- INFERENCE WITH TTA (Per-Frame Minimum Error) ---\nprint(\"ğŸš€ Phase 3: Inference on Test Videos (With Frame-Wise Rotation Check)...\")\ntest_folders = sorted(glob.glob(os.path.join(TEST_DIR, \"*\")))\n\nall_ids = []\nall_errors = []\n\nfor folder in tqdm(test_folders):\n    # 1. READ IMAGES\n    image_paths = sorted(glob.glob(os.path.join(folder, \"*\")))\n    if len(image_paths) == 0: continue\n        \n    frames = []\n    for p in image_paths:\n        img = cv2.imread(p)\n        if img is not None:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            frames.append(img)\n            \n    if len(frames) == 0: continue\n    \n    # 2. PROCESS BATCHES\n    batch_size = 128 # Larger batch since 2D is lighter\n    folder_errors = []\n    \n    for i in range(0, len(frames), batch_size):\n        batch_imgs = frames[i:i+batch_size]\n        \n        # A. Normal Tensor (Upright)\n        t_norm = torch.stack([\n            transform_2d(torch.from_numpy(img).permute(2,0,1)) \n            for img in batch_imgs\n        ]).to(device)\n        \n        # B. Rotated Tensor (Upside Down / 180)\n        # Flip Height (dim 2) and Width (dim 3)\n        t_rot = torch.flip(t_norm, [2, 3])\n        \n        with torch.no_grad():\n            # Extract features for both orientations\n            f_norm = model_2d(t_norm).cpu().numpy()\n            f_rot = model_2d(t_rot).cpu().numpy()\n            \n        # C. Reconstruct Both\n        recon_norm = dae.predict(f_norm, verbose=0)\n        mse_norm = np.mean((f_norm - recon_norm)**2, axis=1)\n        \n        recon_rot = dae.predict(f_rot, verbose=0)\n        mse_rot = np.mean((f_rot - recon_rot)**2, axis=1)\n        \n        # D. MINIMUM LOGIC (The \"Fit\")\n        # For each frame INDEPENDENTLY:\n        # If frame is Upright -> mse_norm is low -> picked.\n        # If frame is Inverted -> mse_rot is low -> picked.\n        # If Anomaly -> Both are high.\n        batch_mse = np.minimum(mse_norm, mse_rot)\n        \n        folder_errors.extend(batch_mse)\n\n    # 3. Store IDs (Paper Format)\n    for p, score in zip(image_paths, folder_errors):\n        parts = p.split(os.sep)\n        # Extract folder number and file number\n        f_num = int(re.findall(r'\\d+', parts[-2])[-1])\n        i_num = int(re.findall(r'\\d+', parts[-1])[-1])\n        clean_id = f\"{f_num}_{i_num}\"\n        \n        all_ids.append(clean_id)\n        all_errors.append(score)\n\n# --- SAVE ---\nsubmission_df = pd.DataFrame({\n    'Id': all_ids,\n    # Standard Rank Normalization only (No PSNR/Gaussian as requested)\n    'Predicted': rankdata(all_errors) / len(all_errors)\n})\n\n# Sort to ensure correct order\nsubmission_df[['Video', 'Frame']] = submission_df['Id'].str.split('_', expand=True).astype(int)\nsubmission_df = submission_df.sort_values(['Video', 'Frame'])\nsubmission_df = submission_df.drop(columns=['Video', 'Frame'])\n\nsubmission_df.to_csv('submission_paper_semantic_optimized.csv', index=False)\nprint(\"ğŸš€ Optimized Submission Saved!\")\nprint(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T20:58:31.657897Z","iopub.execute_input":"2026-01-05T20:58:31.658470Z","iopub.status.idle":"2026-01-05T21:01:15.031152Z","shell.execute_reply.started":"2026-01-05T20:58:31.658435Z","shell.execute_reply":"2026-01-05T21:01:15.030342Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Phase 3: Inference on Test Videos (With Frame-Wise Rotation Check)...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [02:43<00:00,  7.78s/it]","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Optimized Submission Saved!\n      Id  Predicted\n0  1_939   0.146421\n1  1_940   0.141295\n2  1_941   0.329147\n3  1_942   0.542969\n4  1_943   0.522211\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter1d\n\n# 1. Load your best baseline\ndf = pd.read_csv('/kaggle/working/submission_paper_semantic_optimized.csv')\ndf[['Video', 'Frame']] = df['Id'].str.split('_', expand=True).astype(int)\ndf = df.sort_values(['Video', 'Frame'])\n\n# 2. MSE -> PSNR Conversion\n# Peak Signal-to-Noise Ratio focuses on the reconstruction quality\nepsilon = 1e-10\ndf['PSNR'] = 10 * np.log10(1.0 / (df['Predicted'] + epsilon))\n\n# 3. Choose your Normalization Strategy\n# If Per-Video lowered your score before, try 'GLOBAL'\nSTRATEGY = 'GLOBAL' \n\nif STRATEGY == 'PER_VIDEO':\n    def norm_func(x):\n        return 1 - ((x - x.min()) / (x.max() - x.min() + epsilon))\n    df['Anomaly_Score'] = df.groupby('Video')['PSNR'].transform(norm_func)\nelse:\n    # Global normalization preserves the 'intensity' difference between videos\n    p_min, p_max = df['PSNR'].min(), df['PSNR'].max()\n    df['Anomaly_Score'] = 1 - ((df['PSNR'] - p_min) / (p_max - p_min + epsilon))\n\n# 4. Gaussian Smoothing (More precise than Moving Average)\n# sigma=1.0 or 1.5 is the 'sweet spot' for 24-30fps video\ndf['Smoothed'] = df.groupby('Video')['Anomaly_Score'].transform(\n    lambda x: gaussian_filter1d(x, sigma=4)\n)\n\n# 5. Save the two best candidates\ndf[['Id', 'Anomaly_Score']].rename(columns={'Anomaly_Score': 'Predicted'}).to_csv('sub_psnr_only.csv', index=False)\ndf[['Id', 'Smoothed']].rename(columns={'Smoothed': 'Predicted'}).to_csv('sub_psnr_gaussian.csv', index=False)\n\nprint(\"Generated 'sub_psnr_only.csv' and 'sub_psnr_gaussian.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T21:01:49.533540Z","iopub.execute_input":"2026-01-05T21:01:49.534095Z","iopub.status.idle":"2026-01-05T21:01:49.614822Z","shell.execute_reply.started":"2026-01-05T21:01:49.534067Z","shell.execute_reply":"2026-01-05T21:01:49.614055Z"}},"outputs":[{"name":"stdout","text":"Generated 'sub_psnr_only.csv' and 'sub_psnr_gaussian.csv'\n","output_type":"stream"}],"execution_count":5}]}