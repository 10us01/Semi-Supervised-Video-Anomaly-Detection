{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"},{"sourceId":14355060,"sourceType":"datasetVersion","datasetId":9166312},{"sourceId":14355371,"sourceType":"datasetVersion","datasetId":9166527}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Install YOLO\n!pip install -q ultralytics\n\n# 2. FORCE REPAIR (Fixes the version conflicts)\n!pip install \"protobuf<=3.20.3\" --force-reinstall\n!pip install \"numpy<2.0.0\" --force-reinstall\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport glob\nimport gc\nimport tensorflow as tf\nfrom scipy.ndimage import gaussian_filter1d\n\n# --- 1. GPU CONFIGURATION ---\ntf.config.set_soft_device_placement(True)\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"ðŸš€ GPU Active: {len(gpus)} device(s) found.\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"âš ï¸ WARNING: No GPU found.\")\n\nfrom tensorflow.keras import layers, models, applications, optimizers\n# Import YOLO LAST to avoid memory contention\nfrom ultralytics import YOLO \n\n# --- 2. PATHS & SETTINGS ---\nTRAIN_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/training_videos'\nTEST_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\n\nIMG_SIZE = (128, 128)   # For Scene (Global)\nCROP_SIZE = (64, 64)    # For Objects (Local)\nBATCH_SIZE = 32         # Safe size for dual-model usage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:23:25.340819Z","iopub.execute_input":"2025-12-31T14:23:25.341490Z","iopub.status.idle":"2025-12-31T14:23:47.739323Z","shell.execute_reply.started":"2025-12-31T14:23:25.341461Z","shell.execute_reply":"2025-12-31T14:23:47.738519Z"}},"outputs":[{"name":"stderr","text":"2025-12-31 14:23:27.516172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767191007.714315     120 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767191007.766145     120 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767191008.212012     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191008.212071     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191008.212074     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767191008.212077     120 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ GPU Active: 1 device(s) found.\nCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(\"âš™ï¸ Loading Models...\")\n\n# 1. ResNet50 (Global Feature Extractor)\nresnet = applications.ResNet50(weights='imagenet', include_top=False, pooling='avg')\nresnet.trainable = False\nprint(\"âœ… ResNet50 Loaded.\")\n\n# 2. YOLOv8 (Local Object Detector)\nyolo = YOLO('yolov8n.pt')\nprint(\"âœ… YOLOv8 Loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:23:58.315844Z","iopub.execute_input":"2025-12-31T14:23:58.316448Z","iopub.status.idle":"2025-12-31T14:24:07.805165Z","shell.execute_reply.started":"2025-12-31T14:23:58.316412Z","shell.execute_reply":"2025-12-31T14:24:07.804500Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸ Loading Models...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767191038.469210     120 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\nâœ… ResNet50 Loaded.\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 150.8MB/s 0.0s\nâœ… YOLOv8 Loaded.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def build_sigmoid_dae(input_dim):\n    \"\"\"\n    Sigmoid-based Autoencoder.\n    Ideal for normalized features [0, 1].\n    \"\"\"\n    inputs = layers.Input(shape=(input_dim,))\n    \n    # Noise Layer (Forces model to learn structure, not just copy pixels)\n    x = layers.GaussianNoise(0.1)(inputs)\n    \n    # Encoder\n    x = layers.Dense(1024, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(512, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Bottleneck (Compression)\n    encoded = layers.Dense(32, activation='sigmoid')(x)\n    \n    # Decoder\n    x = layers.Dense(512, activation='sigmoid')(encoded)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(1024, activation='sigmoid')(x)\n    x = layers.BatchNormalization()(x)\n    \n    # Output\n    outputs = layers.Dense(input_dim, activation='sigmoid')(x)\n    \n    model = models.Model(inputs, outputs)\n    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='mse')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:24:12.366542Z","iopub.execute_input":"2025-12-31T14:24:12.367612Z","iopub.status.idle":"2025-12-31T14:24:12.374118Z","shell.execute_reply.started":"2025-12-31T14:24:12.367539Z","shell.execute_reply":"2025-12-31T14:24:12.373370Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"ðŸš€ Phase 1: Global Stream (Scene)...\")\n\ndef extract_global_features(base_dir):\n    folders = sorted([f for f in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(f)])\n    if not folders:\n        print(\"âŒ Error: No folders found.\"); return np.empty((0, 2048))\n    \n    all_features = []\n    print(f\"   Processing {len(folders)} folders...\")\n    \n    for folder in folders:\n        images = sorted(glob.glob(os.path.join(folder, \"*.jpg\")) + glob.glob(os.path.join(folder, \"*.JPG\")))\n        batch = []\n        for img_path in images:\n            img = cv2.imread(img_path)\n            if img is None: continue\n            img = cv2.resize(img, IMG_SIZE)\n            img = applications.resnet50.preprocess_input(img.astype('float32'))\n            batch.append(img)\n            \n            if len(batch) >= BATCH_SIZE:\n                # Use predict_on_batch for stability\n                feats = resnet.predict_on_batch(np.array(batch))\n                all_features.append(feats)\n                batch = []\n        \n        if batch:\n            feats = resnet.predict_on_batch(np.array(batch))\n            all_features.append(feats)\n        print(f\"   -> Scene: {os.path.basename(folder)} done.\")\n            \n    return np.vstack(all_features) if all_features else np.empty((0, 2048))\n\n# Execute\nglobal_train_data = extract_global_features(TRAIN_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:24:16.667311Z","iopub.execute_input":"2025-12-31T14:24:16.668096Z","iopub.status.idle":"2025-12-31T14:27:36.767612Z","shell.execute_reply.started":"2025-12-31T14:24:16.668064Z","shell.execute_reply":"2025-12-31T14:27:36.766672Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Phase 1: Global Stream (Scene)...\n   Processing 16 folders...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1767191059.872882     180 service.cc:152] XLA service 0x7c5b140056b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1767191059.872924     180 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1767191060.673310     180 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1767191063.356918     180 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"   -> Scene: 01 done.\n   -> Scene: 02 done.\n   -> Scene: 03 done.\n   -> Scene: 04 done.\n   -> Scene: 05 done.\n   -> Scene: 06 done.\n   -> Scene: 07 done.\n   -> Scene: 08 done.\n   -> Scene: 09 done.\n   -> Scene: 10 done.\n   -> Scene: 11 done.\n   -> Scene: 12 done.\n   -> Scene: 13 done.\n   -> Scene: 14 done.\n   -> Scene: 15 done.\n   -> Scene: 16 done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(\"ðŸš€ Phase 2: Local Stream (Objects)...\")\n\ndef extract_local_features(base_dir):\n    folders = sorted([f for f in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(f)])\n    local_features = []\n    \n    for folder in folders:\n        images = sorted(glob.glob(os.path.join(folder, \"*.jpg\")) + glob.glob(os.path.join(folder, \"*.JPG\")))\n        \n        # We accumulate CROPS, not frames, to batch them efficiently\n        crop_batch = []\n        \n        for img_path in images:\n            img = cv2.imread(img_path)\n            if img is None: continue\n            \n            # 1. Run YOLO (Fast Mode)\n            results = yolo(img, verbose=False, classes=[0]) # 0 = Person\n            \n            # 2. Process Detections\n            for res in results:\n                boxes = res.boxes.xyxy.cpu().numpy()\n                for box in boxes:\n                    x1, y1, x2, y2 = map(int, box)\n                    \n                    # Pad the crop slightly\n                    h, w = img.shape[:2]\n                    x1, y1 = max(0, x1-5), max(0, y1-5)\n                    x2, y2 = min(w, x2+5), min(h, y2+5)\n                    \n                    crop = img[y1:y2, x1:x2]\n                    if crop.size > 0:\n                        crop = cv2.resize(crop, CROP_SIZE)\n                        crop = applications.resnet50.preprocess_input(crop.astype('float32'))\n                        crop_batch.append(crop)\n            \n            # 3. If we have enough crops, run ResNet\n            if len(crop_batch) >= BATCH_SIZE:\n                feats = resnet.predict_on_batch(np.array(crop_batch))\n                local_features.append(feats)\n                crop_batch = [] # Reset\n        \n        # Cleanup remaining crops\n        if crop_batch:\n            feats = resnet.predict_on_batch(np.array(crop_batch))\n            local_features.append(feats)\n            \n        print(f\"   -> Objects: {os.path.basename(folder)} done.\")\n        \n    return np.vstack(local_features) if local_features else np.empty((0, 2048))\n\n# Execute\nlocal_train_data = extract_local_features(TRAIN_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:28:32.780597Z","iopub.execute_input":"2025-12-31T14:28:32.781345Z","iopub.status.idle":"2025-12-31T14:32:28.085406Z","shell.execute_reply.started":"2025-12-31T14:28:32.781313Z","shell.execute_reply":"2025-12-31T14:32:28.084585Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Phase 2: Local Stream (Objects)...\n   -> Objects: 01 done.\n   -> Objects: 02 done.\n   -> Objects: 03 done.\n   -> Objects: 04 done.\n   -> Objects: 05 done.\n   -> Objects: 06 done.\n   -> Objects: 07 done.\n   -> Objects: 08 done.\n   -> Objects: 09 done.\n   -> Objects: 10 done.\n   -> Objects: 11 done.\n   -> Objects: 12 done.\n   -> Objects: 13 done.\n   -> Objects: 14 done.\n   -> Objects: 15 done.\n   -> Objects: 16 done.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- 1. Train Global DAE ---\nif len(global_train_data) > 0:\n    g_min, g_max = global_train_data.min(), global_train_data.max()\n    global_train_data = (global_train_data - g_min) / (g_max - g_min + 1e-7)\n    \n    print(\"\\nðŸ§  Training Global DAE...\")\n    global_dae = build_sigmoid_dae(2048)\n    global_dae.fit(global_train_data, global_train_data, epochs=50, batch_size=128, verbose=1)\nelse:\n    print(\"âŒ No Global Data!\")\n\n# --- 2. Train Local DAE ---\nif len(local_train_data) > 0:\n    l_min, l_max = local_train_data.min(), local_train_data.max()\n    local_train_data = (local_train_data - l_min) / (l_max - l_min + 1e-7)\n    \n    print(\"\\nðŸ§  Training Local DAE...\")\n    local_dae = build_sigmoid_dae(2048)\n    local_dae.fit(local_train_data, local_train_data, epochs=50, batch_size=128, verbose=1)\nelse:\n    print(\"âš ï¸ No Objects detected. Local DAE will be disabled.\")\n    local_dae = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:33:10.105473Z","iopub.execute_input":"2025-12-31T14:33:10.106049Z","iopub.status.idle":"2025-12-31T14:35:37.812463Z","shell.execute_reply.started":"2025-12-31T14:33:10.106019Z","shell.execute_reply":"2025-12-31T14:35:37.811841Z"}},"outputs":[{"name":"stdout","text":"\nðŸ§  Training Global DAE...\nEpoch 1/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.2301\nEpoch 2/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2087\nEpoch 3/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1809\nEpoch 4/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1199\nEpoch 5/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0608\nEpoch 6/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0289\nEpoch 7/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0151\nEpoch 8/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0090\nEpoch 9/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059\nEpoch 10/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042\nEpoch 11/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0031\nEpoch 12/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0024\nEpoch 13/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020\nEpoch 14/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017\nEpoch 15/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014\nEpoch 16/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0012\nEpoch 17/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011\nEpoch 18/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.8120e-04\nEpoch 19/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9169e-04\nEpoch 20/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1203e-04\nEpoch 21/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.5074e-04\nEpoch 22/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.9903e-04\nEpoch 23/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5183e-04\nEpoch 24/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.1368e-04\nEpoch 25/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.8123e-04\nEpoch 26/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5518e-04\nEpoch 27/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2953e-04\nEpoch 28/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.0428e-04\nEpoch 29/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.8421e-04\nEpoch 30/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6643e-04\nEpoch 31/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5062e-04\nEpoch 32/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3673e-04\nEpoch 33/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2058e-04\nEpoch 34/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0956e-04\nEpoch 35/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0056e-04\nEpoch 36/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9054e-04\nEpoch 37/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8097e-04\nEpoch 38/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7004e-04\nEpoch 39/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6316e-04\nEpoch 40/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5782e-04\nEpoch 41/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5080e-04\nEpoch 42/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4105e-04\nEpoch 43/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3786e-04\nEpoch 44/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3148e-04\nEpoch 45/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3056e-04\nEpoch 46/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2804e-04\nEpoch 47/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1804e-04\nEpoch 48/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1669e-04\nEpoch 49/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1428e-04\nEpoch 50/50\n\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0775e-04\n\nðŸ§  Training Local DAE...\nEpoch 1/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.1895\nEpoch 2/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0049\nEpoch 3/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0012\nEpoch 4/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.6267e-04\nEpoch 5/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.7511e-04\nEpoch 6/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.8780e-04\nEpoch 7/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.3868e-04\nEpoch 8/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.1287e-04\nEpoch 9/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.8945e-04\nEpoch 10/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.7793e-04\nEpoch 11/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.7185e-04\nEpoch 12/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.6329e-04\nEpoch 13/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.6031e-04\nEpoch 14/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.6045e-04\nEpoch 15/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5563e-04\nEpoch 16/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5228e-04\nEpoch 17/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5473e-04\nEpoch 18/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4993e-04\nEpoch 19/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4960e-04\nEpoch 20/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5050e-04\nEpoch 21/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4672e-04\nEpoch 22/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4375e-04\nEpoch 23/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4227e-04\nEpoch 24/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4262e-04\nEpoch 25/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4131e-04\nEpoch 26/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4037e-04\nEpoch 27/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3979e-04\nEpoch 28/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3913e-04\nEpoch 29/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4215e-04\nEpoch 30/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3771e-04\nEpoch 31/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3591e-04\nEpoch 32/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3495e-04\nEpoch 33/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3496e-04\nEpoch 34/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3465e-04\nEpoch 35/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3273e-04\nEpoch 36/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3251e-04\nEpoch 37/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3283e-04\nEpoch 38/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3061e-04\nEpoch 39/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2974e-04\nEpoch 40/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2887e-04\nEpoch 41/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2888e-04\nEpoch 42/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2822e-04\nEpoch 43/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2666e-04\nEpoch 44/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2594e-04\nEpoch 45/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2605e-04\nEpoch 46/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2479e-04\nEpoch 47/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2421e-04\nEpoch 48/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2447e-04\nEpoch 49/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2379e-04\nEpoch 50/50\n\u001b[1m589/589\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.2255e-04\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport re  \nimport numpy as np\nimport pandas as pd\nimport shutil\nfrom scipy.ndimage import gaussian_filter1d\nfrom joblib import Parallel, delayed\nfrom IPython.display import FileLink\n\n# ==========================================\n# 1. SAVE MODELS (Safety Step)\n# ==========================================\nprint(\"ðŸ’¾ Saving Models...\")\nsave_dir = 'saved_models'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\ntry:\n    if 'global_dae' in globals(): global_dae.save(os.path.join(save_dir, 'global_dae.h5'))\n    if 'local_dae' in globals(): local_dae.save(os.path.join(save_dir, 'local_dae.h5'))\n    shutil.make_archive('my_models', 'zip', save_dir)\n    print(\"âœ… Models saved.\")\n    display(FileLink('my_models.zip'))\nexcept Exception as e:\n    print(f\"âš ï¸ Warning: Could not save models: {e}\")\n\n# ==========================================\n# 2. INFERENCE WITH ROBUST REGEX PARSING\n# ==========================================\nprint(\"\\nðŸš€ Starting Turbo Inference (Regex ID Parsing)...\")\n\nTEST_DIR = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos\"\nINF_BATCH = 32\nresults = []\n\n# Helper function from your example code\ndef get_clean_number(text):\n    # Finds all digit sequences and takes the last one (e.g. \"Test001\" -> 1)\n    # int() automatically removes leading zeros (001 -> 1)\n    matches = re.findall(r'\\d+', text)\n    if matches:\n        return int(matches[-1])\n    return None\n\ntest_folders = sorted([f for f in glob.glob(os.path.join(TEST_DIR, \"*\")) if os.path.isdir(f)])\n\nfor folder in test_folders:\n    folder_name = os.path.basename(folder)\n    \n    # --- REGEX PARSE VIDEO ID ---\n    vid_num = get_clean_number(folder_name)\n    if vid_num is None: continue\n\n    # Get Images\n    images = sorted(glob.glob(os.path.join(folder, \"*.jpg\")) + \n                    glob.glob(os.path.join(folder, \"*.JPG\")) + \n                    glob.glob(os.path.join(folder, \"*.tif\")))\n    \n    print(f\"   Processing Video {vid_num} ({len(images)} frames)...\")\n\n    # Batched Loop\n    for i in range(0, len(images), INF_BATCH):\n        batch_paths = images[i : i + INF_BATCH]\n        if not batch_paths: continue\n\n        batch_frames = []\n        batch_ids = []\n        \n        for path in batch_paths:\n            img = cv2.imread(path)\n            if img is not None:\n                img_resized = cv2.resize(img, (224, 224))\n                batch_frames.append(img_resized)\n                \n                # --- REGEX PARSE FRAME ID ---\n                # Extracts '00939' from 'frame_00939.tif' and converts to 939\n                fname = os.path.basename(path)\n                frame_num = get_clean_number(fname)\n                \n                # Create Strict ID: \"1_939\"\n                batch_ids.append(f\"{vid_num}_{frame_num}\")\n\n        if not batch_frames: continue\n        \n        # Predict (Your Model Logic)\n        g_batch = np.array(batch_frames).astype('float32')\n        g_batch = applications.resnet50.preprocess_input(g_batch)\n\n        g_feats = resnet.predict_on_batch(g_batch)\n        g_feats_norm = (g_feats - g_min) / (g_max - g_min + 1e-7)\n        g_recons = global_dae.predict_on_batch(g_feats_norm)\n        g_mses = np.mean(np.square(g_feats_norm - g_recons), axis=1)\n\n        # Store\n        for j in range(len(batch_frames)):\n            results.append({\n                'Id': batch_ids[j],      # e.g., \"1_939\"\n                'Video': vid_num,\n                'Raw_Error': g_mses[j]\n            })\n\n# ==========================================\n# 3. SMOOTHING & SAVE\n# ==========================================\nprint(\"âš¡ Smoothing & Formatting...\")\ndf = pd.DataFrame(results)\n\n# Calc Scores\nmax_err = df['Raw_Error'].max()\ndf['PSNR'] = 10 * np.log10((max_err**2) / (df['Raw_Error'] + 1e-10))\np_min, p_max = df['PSNR'].min(), df['PSNR'].max()\ndf['Anomaly_Score'] = 1 - ((df['PSNR'] - p_min) / (p_max - p_min + 1e-10))\n\n# Parallel Smoothing\ndef smooth_chunk(idx, scores):\n    return idx, gaussian_filter1d(scores, sigma=4)\n\nsmoothed_results = Parallel(n_jobs=-1)(\n    delayed(smooth_chunk)(grp.index, grp['Anomaly_Score'].values)\n    for _, grp in df.groupby('Video')\n)\n\ndf['Smoothed'] = 0.0\nfor idx, val in smoothed_results:\n    df.loc[idx, 'Smoothed'] = val\n\n# Save Final\ndf[['Id', 'Smoothed']].rename(columns={'Smoothed': 'Predicted'}).to_csv('submission.csv', index=False)\n\nprint(f\"âœ… DONE! Saved 'submission.csv'\")\nprint(f\"   Sample ID Check: {df.iloc[-1]['Id']} (Should be formatted like '21_120' etc.)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:36:37.473918Z","iopub.execute_input":"2025-12-31T14:36:37.474884Z","iopub.status.idle":"2025-12-31T14:41:23.788134Z","shell.execute_reply.started":"2025-12-31T14:36:37.474852Z","shell.execute_reply":"2025-12-31T14:41:23.787345Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"ðŸ’¾ Saving Models...\nâœ… Models saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/my_models.zip","text/html":"<a href='my_models.zip' target='_blank'>my_models.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"\nðŸš€ Starting Turbo Inference (Regex ID Parsing)...\n   Processing Video 1 (499 frames)...\n   Processing Video 2 (1211 frames)...\n   Processing Video 3 (737 frames)...\n   Processing Video 4 (947 frames)...\n   Processing Video 5 (1007 frames)...\n   Processing Video 6 (627 frames)...\n   Processing Video 7 (588 frames)...\n   Processing Video 8 (36 frames)...\n   Processing Video 9 (359 frames)...\n   Processing Video 10 (722 frames)...\n   Processing Video 11 (472 frames)...\n   Processing Video 12 (735 frames)...\n   Processing Video 13 (528 frames)...\n   Processing Video 14 (496 frames)...\n   Processing Video 15 (732 frames)...\n   Processing Video 16 (740 frames)...\n   Processing Video 17 (417 frames)...\n   Processing Video 18 (275 frames)...\n   Processing Video 19 (229 frames)...\n   Processing Video 20 (273 frames)...\n   Processing Video 21 (76 frames)...\nâš¡ Smoothing & Formatting...\nâœ… DONE! Saved 'submission.csv'\n   Sample ID Check: 21_75 (Should be formatted like '21_120' etc.)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter1d\n\n# 1. Load your best baseline\ndf = pd.read_csv('/kaggle/input/res-yo/submission (1).csv')\ndf[['Video', 'Frame']] = df['Id'].str.split('_', expand=True).astype(int)\ndf = df.sort_values(['Video', 'Frame'])\n\n# 2. MSE -> PSNR Conversion\n# Peak Signal-to-Noise Ratio focuses on the reconstruction quality\nepsilon = 1e-10\ndf['PSNR'] = 10 * np.log10(1.0 / (df['Predicted'] + epsilon))\n\n# 3. Choose your Normalization Strategy\n# If Per-Video lowered your score before, try 'GLOBAL'\nSTRATEGY = 'PER_VIDEO' \n\nif STRATEGY == 'PER_VIDEO':\n    def norm_func(x):\n        return 1 - ((x - x.min()) / (x.max() - x.min() + epsilon))\n    df['Anomaly_Score'] = df.groupby('Video')['PSNR'].transform(norm_func)\nelse:\n    # Global normalization preserves the 'intensity' difference between videos\n    p_min, p_max = df['PSNR'].min(), df['PSNR'].max()\n    df['Anomaly_Score'] = 1 - ((df['PSNR'] - p_min) / (p_max - p_min + epsilon))\n\n# 4. Gaussian Smoothing (More precise than Moving Average)\n# sigma=1.0 or 1.5 is the 'sweet spot' for 24-30fps video\ndf['Smoothed'] = df.groupby('Video')['Anomaly_Score'].transform(\n    lambda x: gaussian_filter1d(x, sigma=1.2)\n)\n\n# 5. Save the two best candidates\ndf[['Id', 'Anomaly_Score']].rename(columns={'Anomaly_Score': 'Predicted'}).to_csv('sub_psnr_only.csv', index=False)\ndf[['Id', 'Smoothed']].rename(columns={'Smoothed': 'Predicted'}).to_csv('sub_psnr_gaussian.csv', index=False)\n\nprint(\"Generated 'sub_psnr_only.csv' and 'sub_psnr_gaussian.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T14:48:42.044546Z","iopub.execute_input":"2025-12-31T14:48:42.044921Z","iopub.status.idle":"2025-12-31T14:48:44.589781Z","shell.execute_reply.started":"2025-12-31T14:48:42.044881Z","shell.execute_reply":"2025-12-31T14:48:44.588755Z"}},"outputs":[{"name":"stdout","text":"Generated 'sub_psnr_only.csv' and 'sub_psnr_gaussian.csv'\n","output_type":"stream"}],"execution_count":1}]}